# ZooKeeper

## Модель данных

- Иерархическое key-value хранилище
- Ключи образуют дерево
- Значения - произвольные байтовые строки
- Каждый узел хранит значение и может иметь детей (одновременно файл и директория в терминах файловых систем)

## Команды

- `create(path, data)`
- `delete(path, version)`
- `exists(path)`
- `getData(path)`
- `setData(p, d, version)`
- `getChildren(path)`
- `sync()`

Аргумент `version` в модифицирующих запросах играет роль `expected` в конструкции Compare-And-Set.

Если версия совпала с версией узла - операция будет совершена, иначе будет проигнорирована. `version = -1` делает операцию безусловной.

Версия узла увеличивается при каждом успешном изменении.

Однако, при удалении и создании узла заново, его версия будет нулевой, что порождает ABA-problem (!).

## Типы узлов

Обычный узел:

- `name := create(/path/to/node_)`
- Ничего особенного.

Последовательные узлы:

- `name := create(/path/to/node_, SEQUENTIAL)`
- Узел будет создан всегда, и его имя будет состоять из заданного префикса и последовательно увеличивающегося числа.

Эфемерные узлы:

- `create(/path/to/node, EPHEMERAL)`
- Существуют только пока создавший их клиент подключён к ZooKeeper (пока шлёт хартбиты).

Эфемерные узлы не могут иметь детей (иначе необходимо бы было уметь как-то согласованно и атомарно удалять целые поддеревья?), но могут хранить данные.

Эфемерные-последовательные узлы:

- `create(/path/to/node, EPHEMERAL_SEQUENTIAL)`

Очевидно, что это делает.

## Уведомления

ZooKeeper поддерживает уведомления, как способ информирования клиента об изменении узла:

- Узел появился
- Узел удалён
- У узла изменился список детей
- Изменились данные, которые хранит узел

При этом в уведомлении не приходит информация о том, как именно изменился узел, например, какие данные в него записаны.

Уведомление сбрасывается после срабатывания, поэтому его необходимо переустанавливать вручную.

Сначала клиент увидит уведомление об изменении, и только потом сможет увидеть само изменение, поскольку уведомления и результаты используют один FIFO-канал.

Но на картине нарисовано иначе?????

## Примитивы координации

### Состав кластера

- Создаём эфемерный узел `/members/node_id`
- При отключении узла от кластера узел автоматически удаляется
- Подписываемся на изменение списка детей `/members` чтобы узнать, когда состав кластера меняется

### Конфигурация

- Подписываемся на изменение списка детей `/conf` чтобы узнать, когда появляются новые элементы конфигурации
- Подписываемся на изменение значения `/conf/port` чтобы узнать, когда поменяется порт
- Подписываемся на изменение значения `/conf/token` чтобы узнать, когда поменяется токен
- Подписываемся на изменение значения ... и т.д.

### Барьеры

- Есть K процессов
- В произвольный момент времени каждый может заявить о своей готовности начать работу. После этого процесс ждёт остальных
- Начинаем работать как только все готовы

Создаем путь `/barrier/` и подписываемся на изменение количества детей. Когда процесс готов, он создаем свою ноду в `/barrier`, все процессы получают уведомление и проверяют, что размер равен $K$, и если истина, то барьер пройден.

### Наивная блокировка

Пытаемся создать путь `/locks/lock_id`. Если получилось, бокировка у нас и для разблокировки нужно удалить ноду. Если не получилось, подписываемся на удаление ноды и пробуем взять блокировку заново.

Недостаток: после отпускания блокировки все процессы одновременно начнут ломиться в ZooKeeper.

### Хорошая блокировка

- Выстраиваем желающих получить блокировку в очередь
- Каждому выдаём номер билета - монотонно возрастающее число
- Блокировку берём в порядке, заданном номерами

Мы хотим, чтобы как только предыдущий отпускал блокировку, мы просыпались. Для этого воспользуемся последовательными узлами.

По пути `locks/lock_id` создаем свой последовательный узел. Если мы уже первые, то блокировка наша, а если до нас еще кто-то есть, мы подписываемся на удаление этого узла. Как только приходит уведомление, повторяем проверку.

Разблокировка тривиальная.

Недосаток: если один из клиентов отвалится, нода будет висеть вечность, и прогресса в системе не будет.

### Блокировка с прогрессом

Используем вместо просто последовательной ноды эфемерную-последовательную.

Таким образом при сбое клиента нода будет удалена ZooKeeper'ом и следующий пойдет за блокировкой или продвинется в очереди.

Проблема:

Клиент может быть жив, но его сообщения могут не доходить до ZooKeeper. Клиент считает, что у него всё ещё есть блокировка.

- Решаем через старые-добрые leases
- Клиент считает, что если он уже T секунд не получал от ZooKeeper ответы на хартбиты, то его отключило от сервера. Значит, он лишился блокировки
- ZooKeeper считает клиента отключенным если хартбиты от него не приходили уже `2 * T` секунд

Отсюда следует, что клиент может лишиться блокировки, выполнив только
часть действий, и получим систему в неконсистентном состоянии. Надо как-то с этим жить.

### Выбор лидера

- Эквивалентен блокировке (у кого блокировка, тот и лидер)
- Узлы должны быть эфемерные, чтобы умерший лидер переставал быть лидером

### Read/Write блокировка

- Для каждого узла известен его тип: R или W
- Писатель просто следит за стоящим перед ним в очереди и берет блокировку как только становится первым
- Читатель следит за состоянием ближайшего к нему в очереди писателя и заходит, как только перед ним в очереди нет писателей

## Распределённый ZooKeeper

- Идейно очень похоже на Raft
- Есть лидер и фолловеры
- Закомиченные и незакомиченные данные
- Использует репликацию журнала
- По журналу строим локальную структуру данных

Клиент подключен к единственному узлу и все команды направляет на этот узел. Только этот узел посылает клиенту уведомления. Клиент переподключается к другому узлу только при сбоях.

### Чтение

- Запросы на чтение работают локально
- Сервер смотрит в локальную структуру данных и отвечает
- Можно прочитать устаревшие данные

Команда `Sync()` позволяет синхронизовать лидера и реплику. На момент коммита sync у нас в логе есть все предыдущие записи.

Значит ли это, что использование `Sync()` перед `GetData()` позволит нам прочитать самые свежие данные? Нет, так как между командами могло произойти что-то еще.

Мы помним, что для получения актуальных данных чтение нужно делать через кворум. ZooKeeper такое не поддерживает.

В блокировках, кстати, неимение самой последней версии ничем страшным не грозит - ведь нам важны только узлы до нас, а про них мы знаем, потому что репликация работает по префиксу.

### Запись

Сперва клиент делает запись на узел, к которому он подключён, затем узел пересылает запись лидеру.

Лидер добавляет её в лог и помечает как незакоммиченную, затем реплицирует запись на кворум узлов (пока что запись у всех не закоммичена). Как только эта запись появляется у кворума узлов, лидер помечает запись как закоммиченную.

Лидер сообщает узлам о том, что эта запись закоммичена (в том числе и узлу, к которому подключён клиент), а узел сообщает клиенту, что его запрос выполнен.

### Сбои узлов

При сбое узлов клиент подключается к другому узлу, при этом у нового узла лог должен быть не короче, чем у предыдущего, чтобы чтение было монотонным.

При этом, уведомления также должны обработаться корректно, ведь часть из них могла сработать из-за операций на суффиксе, которые старый узел не знал, поэтому мы сообщаем об нужных уведомлениях клиенту перед началом нормальной работы (перед посылкой обчных уведомлений).

но что если клиент будет читать уведомления и делать запросы getdata????? какая-то чушь получится с его стороны

### Эфемерные узлы

- Лидер следит за состоянием клиентов, которые создали эфемерные узлы
- Удаляет эфемерные узлы, если долго не было хартбитов

Обработка эфемерных узлов лидером необходима по причине возможного состояния разделенной сети.

Клиент может быть подключён к живому серверу, который изолирован от лидера. Когда возникает угроза удаления эфемерных узлов, сервер сообщает об этом клиенту, а тот переподключается к другим серверам или отказывается от блокировки/лидерства/etc.
